import sys
import os
import asyncio

# Windows ä¿®å¤ (ä»…ä¿ç•™è¿™ä¸€è¡Œå…¼å®¹æ€§ä»£ç )
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

import nest_asyncio

nest_asyncio.apply()

import json
from datetime import datetime, timedelta
from sqlmodel import SQLModel, create_engine, Session, select
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import dateparser
from models import SiteConfig, Article, GlobalSettings
from bs4 import BeautifulSoup
from openai import OpenAI
from collections import Counter

sqlite_file_name = "database.db"
sqlite_url = f"sqlite:///{sqlite_file_name}"
engine = create_engine(sqlite_url)

# âš ï¸ å¡«å…¥ä½ çš„ Key
AI_CLIENT = OpenAI(
    api_key="sk-5836d26f5793456d80465828e44b48de",
    base_url="https://api.deepseek.com"
)


def init_db():
    SQLModel.metadata.create_all(engine)
    with Session(engine) as session:
        if not session.exec(select(GlobalSettings)).first():
            session.add(GlobalSettings())
            session.commit()


def ensure_http(url: str) -> str:
    if not url: return ""
    url = url.strip()
    if not url.startswith("http://") and not url.startswith("https://"):
        return "https://" + url
    return url


def parse_date_smart(date_str: str, format_str: str = None):
    if not date_str: return None
    date_str = date_str.strip()
    if format_str:
        try:
            return datetime.strptime(date_str, format_str)
        except:
            pass
    return dateparser.parse(date_str)


# === æ ¸å¿ƒï¼šæ‰¹é‡çˆ¬å–é€»è¾‘ (æ”¯æŒ {n} åˆ†é¡µ) ===
async def crawl_all_sites(site_ids: list, days_back: int, max_pages: int):
    stats = {"total_crawled": 0, "new_added": 0, "duplicates": 0, "details": []}

    with Session(engine) as session:
        cutoff_date = datetime.now() - timedelta(days=days_back)
        browser_config = BrowserConfig(headless=False, verbose=True, user_agent_mode="random")

        async with AsyncWebCrawler(config=browser_config) as crawler:
            for site_id in site_ids:
                config = session.get(SiteConfig, site_id)
                if not config: continue
                if not config.is_active:
                    print(f"[SKIP] è·³è¿‡æœªå¯ç”¨: {config.name}")
                    continue

                site_stat = {"name": config.name, "new": 0, "dup": 0}
                base_url = ensure_http(config.url)

                # === åˆ¤æ–­åˆ†é¡µæ¨¡å¼ ===
                # æ¨¡å¼ A: URL åŒ…å« {n} -> æ•°å­—åˆ†é¡µ
                # æ¨¡å¼ B: CSS é€‰æ‹©å™¨ -> åŠ¨æ€ç¿»é¡µ
                is_number_pagination = "{n}" in base_url

                print(f"[INFO] å¼€å§‹çˆ¬å–: {config.name} (æ¨¡å¼: {'æ•°å­—åˆ†é¡µ' if is_number_pagination else 'CSSç¿»é¡µ'})")

                current_url = base_url
                page_num = 1

                while page_num <= max_pages:
                    # 1. ç¡®å®šå½“å‰é¡µ URL
                    if is_number_pagination:
                        current_url = base_url.replace("{n}", str(page_num))
                    # else: CSS æ¨¡å¼ä¸‹ current_url ä¼šåœ¨å¾ªç¯æœ«å°¾æ›´æ–°

                    if not current_url: break

                    print(f"   ğŸ•·ï¸ æŠ“å–ç¬¬ {page_num} é¡µ: {current_url}")

                    # 2. æ„å»ºæå–è§„åˆ™
                    fields = [
                        {"name": "title", "selector": config.title_selector, "type": "text"},
                        {"name": "url", "selector": config.link_selector, "type": "attribute", "attribute": "href"},
                    ]
                    if config.date_selector:
                        fields.append({"name": "date", "selector": config.date_selector, "type": "text"})

                    # åªæœ‰ CSS æ¨¡å¼æ‰éœ€è¦æå–ä¸‹ä¸€é¡µé“¾æ¥
                    if config.next_page_selector and not is_number_pagination:
                        fields.append({"name": "next_page", "selector": config.next_page_selector, "type": "attribute",
                                       "attribute": "href"})

                    schema = {"baseSelector": config.list_selector, "fields": fields}

                    run_config = CrawlerRunConfig(
                        extraction_strategy=JsonCssExtractionStrategy(schema),
                        cache_mode=CacheMode.BYPASS,
                        js_code="window.scrollTo(0, document.body.scrollHeight);",
                        wait_for="body"
                    )

                    result = await crawler.arun(url=current_url, config=run_config)

                    if not result.success:
                        print(f"   [ERROR] é¡µé¢åŠ è½½å¤±è´¥")
                        break

                    try:
                        items = json.loads(result.extracted_content)
                    except:
                        items = []

                    if not items:
                        print(f"   [WARN] æœ¬é¡µæ— æ•°æ®")
                        break

                    next_page_link = None
                    has_valid_date_in_page = False  # æœ¬é¡µæ˜¯å¦æœ‰ç¬¦åˆæ—¶é—´çš„æ•°æ®

                    # 3. å¤„ç†æ•°æ®
                    for item in items:
                        # æå–ä¸‹ä¸€é¡µ (ä»… CSS æ¨¡å¼)
                        if not is_number_pagination and config.next_page_selector and item.get(
                                'next_page') and not next_page_link:
                            next_page_link = item.get('next_page')

                        if not item.get('title') or not item.get('url'): continue

                        full_url = item['url']
                        if not full_url.startswith('http'):
                            from urllib.parse import urljoin
                            full_url = urljoin(current_url, full_url)

                        pub_date = parse_date_smart(item.get('date'), config.date_format)

                        # å®½æ¾è¿‡æ»¤ï¼šå¦‚æœæœ‰æ—¥æœŸä¸”å¤ªæ—§åˆ™è·³è¿‡ï¼›æ— æ—¥æœŸåˆ™ä¿ç•™
                        if pub_date:
                            if pub_date < cutoff_date:
                                continue
                            else:
                                has_valid_date_in_page = True
                        else:
                            has_valid_date_in_page = True  # æ— æ—¥æœŸä¹Ÿç®—æœ‰æ•ˆï¼Œé˜²æ­¢æ¼æŠ“

                        exists = session.exec(select(Article).where(Article.url == full_url)).first()
                        if exists:
                            site_stat["dup"] += 1
                            stats["duplicates"] += 1
                        else:
                            article = Article(
                                site_id=config.id,
                                title=item['title'],
                                url=full_url,
                                publish_date=pub_date
                            )
                            session.add(article)
                            site_stat["new"] += 1
                            stats["new_added"] += 1

                        stats["total_crawled"] += 1

                    session.commit()
                    page_num += 1

                    # 4. ç¿»é¡µåˆ¤æ–­
                    if is_number_pagination:
                        # æ•°å­—æ¨¡å¼ï¼šå¦‚æœæœ¬é¡µå®Œå…¨æ²¡æœ‰ç¬¦åˆæ—¥æœŸçš„æ•°æ®ï¼Œå¯èƒ½åé¢æ›´æ—§äº†ï¼Œå¯ä»¥é€‰æ‹©æå‰åœæ­¢
                        # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åªä¾èµ– max_pages é™åˆ¶ï¼Œæˆ–è€…å¦‚æœæå–åˆ°çš„ items ä¸ºç©ºåˆ™åœæ­¢
                        pass
                    else:
                        # CSS æ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢
                        if next_page_link:
                            if not next_page_link.startswith('http'):
                                from urllib.parse import urljoin
                                next_page_link = urljoin(current_url, next_page_link)
                            current_url = next_page_link
                        else:
                            print("   ğŸ æ— ä¸‹ä¸€é¡µï¼Œåœæ­¢")
                            break

                stats["details"].append(site_stat)

    return stats


# === AI åˆ†æ ===
async def analyze_specific_articles(article_ids: list):
    results = []
    with Session(engine) as session:
        settings = session.exec(select(GlobalSettings)).first()
        if not settings: return 0

        comps = json.loads(settings.competitors_json)
        comp_str = f"CN: {', '.join(comps.get('ä¸­æ–‡å', []))}; EN: {', '.join(comps.get('è‹±æ–‡å', []))}"
        topics_str = ", ".join(json.loads(settings.topics_json))
        categories_dict = json.loads(settings.categories_json)
        categories_str = "\n".join([f"- {k}: {v}" for k, v in categories_dict.items()])
        current_date = datetime.now().strftime("%Y-%m-%d")

        articles = session.exec(select(Article).where(Article.id.in_(article_ids))).all()
        if not articles: return 0

        browser_config = BrowserConfig(headless=False, user_agent_mode="random")

        async with AsyncWebCrawler(config=browser_config) as crawler:
            for article in articles:
                target_url = ensure_http(article.url)
                print(f"[AI] åˆ†æ: {article.title}")

                result = await crawler.arun(url=target_url, cache_mode=CacheMode.BYPASS, magic=True)

                if result.success:
                    article.content_raw = result.markdown
                    content_snippet = result.markdown[:6000]

                    prompt = f"""
                    ä½ æ˜¯æƒ…æŠ¥åˆ†æå¸ˆã€‚ä»Šå¤©æ˜¯ {current_date}ã€‚
                    ã€å®¢æˆ·ç”»åƒã€‘{settings.client_profile}
                    ã€é‡ç‚¹å…³æ³¨ç«äº‰å¯¹æ‰‹ã€‘{comp_str}
                    ã€åˆ†ç±»æ ‡å‡†ã€‘è®®é¢˜: {topics_str}
                    ã€æ–°é—»å†…å®¹ã€‘{content_snippet}
                    è¯·è¿”å› JSONï¼š{{
                        "è®®é¢˜": "...", "ç±»åˆ«": "...", "æ‘˜è¦": "...", 
                        "ä¸­æ–‡æ ‡é¢˜": "...", "è‹±æ–‡æ ‡é¢˜": "...",
                        "è¯„åˆ†": <0-10>, "æ‰“åˆ†ç†ç”±": "...",
                        "è¯„åˆ†è¯¦æƒ…": {{ "æˆ˜ç•¥": 0, "è¡Œä¸š": 0, "æ—¶æ•ˆ": 0, "é£é™©": 0, "è½åœ°": 0 }}
                    }}
                    """
                    try:
                        response = AI_CLIENT.chat.completions.create(
                            model="deepseek-chat",
                            messages=[{"role": "user", "content": prompt}],
                            response_format={"type": "json_object"}
                        )
                        ai_data = json.loads(response.choices[0].message.content)
                        article.ai_topic = ai_data.get("è®®é¢˜")
                        article.ai_category = ai_data.get("ç±»åˆ«")
                        article.ai_summary = ai_data.get("æ‘˜è¦")
                        article.new_title = ai_data.get("ä¸­æ–‡æ ‡é¢˜")
                        article.title_en = ai_data.get("è‹±æ–‡æ ‡é¢˜")
                        article.ai_score = ai_data.get("è¯„åˆ†")
                        article.ai_reasoning = ai_data.get("æ‰“åˆ†ç†ç”±")
                        article.ai_score_details = json.dumps(ai_data.get("è¯„åˆ†è¯¦æƒ…", {}), ensure_ascii=False)
                        article.ai_status = "done"
                    except Exception as e:
                        print(f"[ERROR] AI: {e}")
                        article.ai_status = "error"
                else:
                    article.ai_status = "error"
                session.add(article)
                session.commit()
                results.append(article)
    return len(results)


# === AI è‡ªåŠ¨æ¢æµ‹ ===
async def auto_detect_config(url: str):
    target_url = ensure_http(url)
    print(f"[DETECT] æ¢æµ‹: {target_url}")
    browser_config = BrowserConfig(headless=True)

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=target_url, magic=True, cache_mode=CacheMode.BYPASS)
        if not result.success: return {"error": result.error_message}
        html = result.html

    soup = BeautifulSoup(html, 'html.parser')
    for tag in soup(['script', 'style']): tag.decompose()
    clean_html = str(soup.body)[:30000]

    prompt = f"""
    åˆ†æ HTML æ‰¾å‡ºæ–°é—»åˆ—è¡¨ CSS é€‰æ‹©å™¨ã€‚
    ç‰¹åˆ«ä»»åŠ¡ï¼šè§‚å¯Ÿ HTML é‡Œçš„æ—¥æœŸæ ¼å¼ã€‚
    è¿”å› JSON: list, title, link, date, date_format, next_page.
    HTML: {clean_html}
    """
    try:
        response = AI_CLIENT.chat.completions.create(model="deepseek-chat",
                                                     messages=[{"role": "user", "content": prompt}],
                                                     response_format={"type": "json_object"})
        result_data = json.loads(response.choices[0].message.content)
        # ç¡®ä¿è¿”å›çš„URLåŒ…å«æ­£ç¡®çš„åè®®å‰ç¼€
        result_data["url"] = ensure_http(result_data.get("url", url))
        return result_data
    except Exception as e:
        return {"error": str(e)}


async def test_crawler_config(url, selectors):
    target_url = ensure_http(url)
    print(f"[TEST] æµ‹è¯•: {target_url}")
    browser_config = BrowserConfig(headless=False, verbose=True)
    fields = [{"name": "title", "selector": selectors['title'], "type": "text"},
              {"name": "url", "selector": selectors['link'], "type": "attribute", "attribute": "href"}]
    if selectors.get('date'): fields.append({"name": "date", "selector": selectors['date'], "type": "text"})
    schema = {"baseSelector": selectors['list'], "fields": fields}
    run_config = CrawlerRunConfig(extraction_strategy=JsonCssExtractionStrategy(schema), cache_mode=CacheMode.BYPASS,
                                  js_code="window.scrollTo(0, document.body.scrollHeight);", wait_for="body")
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=target_url, config=run_config)
            if not result.success: return {"success": False, "error": result.error_message}
            items = json.loads(result.extracted_content)
            return {"success": True, "count": len(items), "data": items[:3]}
    except Exception as e:
        return {"success": False, "error": str(e)}


async def test_pagination_logic(url: str, selectors: dict):
    """
    å°è¯•æŠ“å–å‰ 2 é¡µï¼ŒéªŒè¯åˆ†é¡µé…ç½®æ˜¯å¦æ­£ç¡®
    """
    base_url = ensure_http(url)
    print(f"[TEST PAGINATION] å¼€å§‹æµ‹è¯•: {base_url}")

    # åˆ¤æ–­æ¨¡å¼
    is_number_pagination = "{n}" in base_url

    report = {
        "mode": "æ•°å­—åˆ†é¡µ {n}" if is_number_pagination else "CSS æŒ‰é’®ç¿»é¡µ",
        "pages": []
    }

    browser_config = BrowserConfig(headless=False, verbose=True)

    # æ„å»º schema
    fields = [
        {"name": "title", "selector": selectors['title'], "type": "text"}
    ]
    if selectors.get('next_page') and not is_number_pagination:
        fields.append(
            {"name": "next_page", "selector": selectors['next_page'], "type": "attribute", "attribute": "href"})

    schema = {"baseSelector": selectors['list'], "fields": fields}

    run_config = CrawlerRunConfig(
        extraction_strategy=JsonCssExtractionStrategy(schema),
        cache_mode=CacheMode.BYPASS,
        js_code="window.scrollTo(0, document.body.scrollHeight);",
        wait_for="body"
    )

    current_url = base_url
    page_num = 1

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # åªæµ‹å‰ 2 é¡µ
        while page_num <= 2:
            # 1. è®¡ç®— URL
            if is_number_pagination:
                target_url = base_url.replace("{n}", str(page_num))
            else:
                target_url = current_url

            if not target_url:
                report["pages"].append(f"ç¬¬ {page_num} é¡µ: æ— æ³•è·å– URLï¼Œåœæ­¢ã€‚")
                break

            print(f"   Testing Page {page_num}: {target_url}")

            # 2. æŠ“å–
            result = await crawler.arun(url=target_url, config=run_config)

            if not result.success:
                report["pages"].append(f"ç¬¬ {page_num} é¡µ: æŠ“å–å¤±è´¥ ({result.error_message})")
                break

            # 3. åˆ†æç»“æœ
            try:
                items = json.loads(result.extracted_content)
            except:
                items = []

            item_count = len(items)
            first_title = items[0]['title'] if items and items[0].get('title') else "æ— æ ‡é¢˜"

            page_info = {
                "page": page_num,
                "url": target_url,
                "status": "Success",
                "item_count": item_count,
                "first_item": first_title
            }

            # 4. å¯»æ‰¾ä¸‹ä¸€é¡µ (ä»… CSS æ¨¡å¼)
            if not is_number_pagination:
                next_link = None
                for item in items:
                    if item.get('next_page'):
                        next_link = item.get('next_page')
                        break

                if next_link:
                    page_info["next_button_found"] = "âœ… æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥"
                    page_info["next_url_raw"] = next_link
                    # è¡¥å…¨ URL
                    if not next_link.startswith('http'):
                        from urllib.parse import urljoin
                        next_link = urljoin(target_url, next_link)
                    current_url = next_link
                else:
                    page_info["next_button_found"] = "âŒ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ (Selectorå¤±æ•ˆæˆ–æ— æ›´å¤šé¡µ)"
                    current_url = None  # åœæ­¢

            report["pages"].append(page_info)

            # å¦‚æœ CSS æ¨¡å¼æ²¡æ‰¾åˆ°ä¸‹ä¸€é¡µï¼Œå°±ä¸æµ‹ç¬¬ 2 é¡µäº†
            if not is_number_pagination and not current_url:
                break

            page_num += 1

    return report