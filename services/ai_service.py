# services/ai_service.py
import os
import json
import re
import logging
import aiohttp
import asyncio
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from datetime import datetime
import dateutil.parser
from datetime import timedelta

# ç¡®ä¿åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class NewsArticle(BaseModel):
    source: str
    title: str
    url: str
    publish_date: str
    content: str
    content_hash: str


class ContentFetchResult(BaseModel):
    success: bool
    content: str
    error_message: Optional[str] = None
    source: str  # "cached" or "fetched" or "fallback"


class DateExtractionResult(BaseModel):
    """æ—¥æœŸæå–ç»“æœ"""
    success: bool
    date: Optional[str] = None
    confidence: float = 0.0  # 0-1ä¹‹é—´ï¼Œè¡¨ç¤ºæå–çš„å¯ä¿¡åº¦
    method: str = ""  # æå–æ–¹æ³•ï¼š"meta_tag", "json_ld", "text_pattern", "url_pattern"
    error_message: Optional[str] = None


class MockConfig:
    OFFICIAL_TOPICS: list = ["å¥åº·ä¸å®‰å…¨", "æ¸…æ´æŠ€æœ¯æœºé‡", "ç»¿è‰²å»ºç­‘", "åº”å¯¹æ°”å€™å˜åŒ–", "ç”Ÿç‰©å¤šæ ·æ€§", "å…¶ä»–"]
    OFFICIAL_CATEGORIES: dict = {
        "æ”¿ç­–åŠ¨æ€": "æŒ‡ç”±å›½å®¶éƒ¨é—¨å‘å¸ƒçš„ã€ä¸è®®é¢˜ç›¸å…³çš„äº§ä¸šæ”¿ç­–æˆ–å£°æ˜...",
        "å‰æ²¿èµ„è®¯": "åŒ…æ‹¬è®®é¢˜ä¸‹çš„æŠ€æœ¯ä¸åˆ›æ–°åŠ¨æ€ã€å¸‚åœºä¸ç«äº‰åŠ¨æ€...",
        "å¿…è¯»æŠ¥å‘Š": "åªè¦æ˜¯å‘å¸ƒæŠ¥å‘Šå°±æ˜¯è¿™ä¸ªåˆ†ç±»"
    }
    DEFAULT_CLIENT_PROFILE: str = (
        "ä¸­å›½å»ºç­‘å›½é™…æ§è‚¡æœ‰é™å…¬å¸ï¼ˆCSCIï¼‰æ˜¯ä¸€å®¶åœ¨é¦™æ¸¯ä¸Šå¸‚çš„å¤§å‹å»ºç­‘åŠåŸºç¡€è®¾æ–½ç»¼åˆä¼ä¸šï¼Œéš¶å±äºå»ºç­‘ä¸å·¥ç¨‹è¡Œä¸šã€‚"
        "å…¬å¸ä¸šåŠ¡ä¸»è¦åˆ†ä¸ºäº”å¤§æ¿å—ï¼š"
        "**å»ºç­‘ç›¸å…³æŠ•èµ„é¡¹ç›®ï¼ˆ51.1%ï¼Œæ”¶å…¥588.4äº¿æ¸¯å…ƒï¼‰**ï¼šå…¬å¸æœ€å¤§æ”¶å…¥æ¥æºï¼Œä»¥æŠ•èµ„è€…èº«ä»½å‚ä¸åŸå¸‚æ›´æ–°ã€åŸºç¡€è®¾æ–½å’Œæˆ¿åœ°äº§é¡¹ç›®çš„å‰æœŸæŠ•èµ„ä¸å¼€å‘ï¼Œ"
        "é€šè¿‡EPCï¼ˆè®¾è®¡-é‡‡è´­-æ–½å·¥ï¼‰æ¨¡å¼å›æ”¶æŠ•èµ„ï¼Œä¸»è¦åŒ…æ‹¬åŸå¸‚æ›´æ–°æ”¹é€ ã€ä¿éšœæˆ¿å»ºè®¾ã€äº§ä¸šå›­åŒºå¼€å‘ç­‰ï¼Œå…¸å‹é¡¹ç›®å¦‚æ·±åœ³å‰æµ·ç‰‡åŒºåŸå¸‚æ›´æ–°é¡¹ç›®ï¼›"
        "**å»ºç­‘åˆçº¦å·¥ç¨‹ï¼ˆ41.3%ï¼Œæ”¶å…¥475.3äº¿æ¸¯å…ƒï¼‰**ï¼šä¼ ç»Ÿæ ¸å¿ƒä¸šåŠ¡ï¼Œæ‰¿æ¥æ”¿åºœã€ä¼ä¸šæˆ–ç§äººå¼€å‘å•†çš„å»ºç­‘å·¥ç¨‹é¡¹ç›®ï¼Œ"
        "æ¶µç›–æˆ¿å»ºå·¥ç¨‹ï¼ˆä½å®…ã€å†™å­—æ¥¼ã€å•†ä¸šç»¼åˆä½“ï¼‰ã€å…¬å…±å·¥ç¨‹ï¼ˆå­¦æ ¡ã€åŒ»é™¢ã€æ”¿åºœå»ºç­‘ï¼‰ã€åŸºç¡€è®¾æ–½å·¥ç¨‹ï¼ˆé“è·¯ã€éš§é“ã€æ¡¥æ¢ã€è½¨é“äº¤é€šï¼‰ï¼Œ"
        "å…¸å‹é¡¹ç›®å¦‚é¦™æ¸¯å°†å†›æ¾³æ—¥å‡ºåº·åŸä½å®…é¡¹ç›®ï¼›"
        "**å¤–å¢™/ç«‹é¢ä¸šåŠ¡ï¼ˆ3.4%ï¼Œæ”¶å…¥39.4äº¿æ¸¯å…ƒï¼‰**ï¼šä¸“ä¸šåŒ–ç¨‹åº¦è¾ƒé«˜çš„ç»†åˆ†ä¸šåŠ¡ï¼Œä»äº‹å»ºç­‘å¹•å¢™ã€ç«‹é¢ã€ç»ç’ƒå¹•å¢™å’Œé‡‘å±è£…é¥°å·¥ç¨‹çš„è®¾è®¡åˆ¶é€ å®‰è£…ï¼Œ"
        "åŒ…æ‹¬é«˜å±‚å»ºç­‘ç»ç’ƒå¹•å¢™ã€å»ºç­‘ç«‹é¢è£…é¥°ã€ç»¿è‰²èŠ‚èƒ½å¹•å¢™ç­‰ï¼›"
        "**åŸºç¡€è®¾æ–½è¥è¿ï¼ˆ0.63%ï¼Œæ”¶å…¥7.2äº¿æ¸¯å…ƒï¼‰**ï¼šæŠ•èµ„å¹¶è¿è¥å·²å»ºæˆåŸºç¡€è®¾æ–½ï¼Œé€šè¿‡æ”¶è´¹è·å¾—é•¿æœŸç¨³å®šç°é‡‘æµï¼Œ"
        "åŒ…æ‹¬æ”¶è´¹é“è·¯éš§é“æ¡¥æ¢è¿è¥ã€å…¬å…±åœè½¦åœºè¿è¥ã€ç¯ä¿è®¾æ–½æ°´åŠ¡è®¾æ–½è¿è¥ç­‰ï¼›"
        "**å…¶ä»–ä¸šåŠ¡ï¼ˆ3.5%ï¼Œæ”¶å…¥40.7äº¿æ¸¯å…ƒï¼‰**ï¼šåŒ…æ‹¬é¡¹ç›®å’¨è¯¢ã€å·¥ç¨‹ç®¡ç†ã€é€ ä»·å’¨è¯¢ã€å‚æˆ¿é‡å»ºæ”¹é€ ã€å»ºç­‘ææ–™é”€å”®ã€æœºæ¢°è®¾å¤‡ç§Ÿèµã€æŠ•èµ„ç‰©ä¸šç§Ÿèµç­‰è¾…åŠ©æ€§ä¸šåŠ¡ã€‚"
        "ä»åœ°åŸŸåˆ†å¸ƒçœ‹ï¼Œå…¬å¸ä¸šåŠ¡ä¸»è¦é›†ä¸­åœ¨å¤§ä¸­ååœ°åŒºï¼šä¸­å›½å¤§é™†å 54.8%ï¼ˆ604.2äº¿æ¸¯å…ƒï¼‰ï¼Œé¦™æ¸¯å 37.3%ï¼ˆ410.9äº¿æ¸¯å…ƒï¼‰ï¼Œæ¾³é—¨å 8.0%ï¼ˆ87.8äº¿æ¸¯å…ƒï¼‰ã€‚"
        "å…¬å¸ä¾æ‰˜åœ¨åŸå¸‚æ›´æ–°ã€åŸºç¡€è®¾æ–½å»ºè®¾ã€é«˜ç«¯å»ºç­‘ç­‰é¢†åŸŸçš„ä¸°å¯Œç»éªŒï¼Œä»¥åŠæ•°å­—åŒ–å»ºé€ æŠ€æœ¯å’Œè·¨åŒºåŸŸç®¡ç†èƒ½åŠ›ï¼Œ"
        "æŒç»­åœ¨ç»¿è‰²å»ºç­‘ã€æ™ºæ…§åŸå¸‚ã€åŸå¸‚æ›´æ–°ç­‰å‰æ²¿é¢†åŸŸæ·±åŒ–å¸ƒå±€ã€‚"
    )
    CONTENT_MAX_LENGTH: int = 8000
    FETCH_TIMEOUT: int = 30  # ç½‘é¡µæŠ“å–è¶…æ—¶æ—¶é—´
    USE_URL_FETCH: bool = True  # æ˜¯å¦å¯ç”¨URLæŠ“å–
    ENABLE_DATE_EXTRACTION: bool = True  # æ˜¯å¦å¯ç”¨æ·±åº¦æ—¥æœŸæå–


config = MockConfig()

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–AIå®¢æˆ·ç«¯
try:
    from openai import AsyncOpenAI

    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

    if not DEEPSEEK_API_KEY:
        logger.error("âŒ æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        print("âŒ DEEPSEEK_API_KEY æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        AI_CLIENT = None
    else:
        AI_CLIENT = AsyncOpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com/v1"
        )
        logger.info(f"âœ… DeepSeek AI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå¯†é’¥: {DEEPSEEK_API_KEY[:10]}...")
        print(f"âœ… AIæœåŠ¡å°±ç»ªï¼Œå¯†é’¥: {DEEPSEEK_API_KEY[:10]}...")

except ImportError:
    logger.error("openai åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install openai'ã€‚")
    AI_CLIENT = None


async def fetch_webpage_content(url: str) -> ContentFetchResult:
    """
    å¼‚æ­¥æŠ“å–ç½‘é¡µå†…å®¹å¹¶æ¸…ç†
    """
    try:
        timeout = aiohttp.ClientTimeout(total=config.FETCH_TIMEOUT)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            async with session.get(url) as response:
                if response.status != 200:
                    return ContentFetchResult(
                        success=False,
                        content="",
                        error_message=f"HTTP {response.status}",
                        source="fetched"
                    )

                html_content = await response.text()

                # ä½¿ç”¨BeautifulSoupæ¸…ç†HTML
                soup = BeautifulSoup(html_content, 'html.parser')

                # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
                for tag in soup(["script", "style", "nav", "header", "footer", "aside", "advertisement"]):
                    tag.decompose()

                # æå–ä¸»è¦å†…å®¹
                # ä¼˜å…ˆæŸ¥æ‰¾å¸¸è§çš„å†…å®¹å®¹å™¨
                content_selectors = [
                    'article',
                    '.content',
                    '.article-content',
                    '.post-content',
                    '.entry-content',
                    'main',
                    '#content'
                ]

                extracted_content = ""
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        extracted_content = elements[0].get_text(strip=True)
                        break

                # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨bodyå†…å®¹
                if not extracted_content:
                    body = soup.find('body')
                    if body:
                        extracted_content = body.get_text(strip=True)

                # æ¸…ç†æ–‡æœ¬
                lines = extracted_content.split('\n')
                cleaned_lines = [line.strip() for line in lines if line.strip()]
                cleaned_content = '\n'.join(cleaned_lines)

                return ContentFetchResult(
                    success=True,
                    content=cleaned_content[:config.CONTENT_MAX_LENGTH],
                    source="fetched"
                )

    except asyncio.TimeoutError:
        return ContentFetchResult(
            success=False,
            content="",
            error_message="è¯·æ±‚è¶…æ—¶",
            source="fetched"
        )
    except Exception as e:
        return ContentFetchResult(
            success=False,
            content="",
            error_message=str(e),
            source="fetched"
        )


async def extract_date_from_url(url: str, date_selectors: List[str] = None) -> DateExtractionResult:
    """
    ä»å…·ä½“URLä¸­æå–æ—¥æœŸä¿¡æ¯

    Args:
        url: è¦æå–æ—¥æœŸçš„URL
        date_selectors: å¯é€‰çš„æ—¥æœŸé€‰æ‹©å™¨åˆ—è¡¨ï¼Œç”¨äºå®šå‘æŸ¥æ‰¾

    Returns:
        DateExtractionResult: æ—¥æœŸæå–ç»“æœ
    """
    logger.info(f"ğŸ• å¼€å§‹ä»URLæå–æ—¥æœŸ: {url}")

    try:
        timeout = aiohttp.ClientTimeout(total=config.FETCH_TIMEOUT)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            async with session.get(url) as response:
                if response.status != 200:
                    return DateExtractionResult(
                        success=False,
                        error_message=f"HTTP {response.status}"
                    )

                html_content = await response.text()
                soup = BeautifulSoup(html_content, 'html.parser')

                # æ–¹æ³•1: ä¼˜å…ˆä½¿ç”¨é…ç½®çš„æ—¥æœŸé€‰æ‹©å™¨
                if date_selectors:
                    for selector in date_selectors:
                        try:
                            elements = soup.select(selector.strip())
                            for element in elements:
                                date_text = element.get_text(strip=True)
                                if date_text:
                                    parsed_date = parse_date_text(date_text)
                                    if parsed_date:
                                        logger.info(f"âœ… é€šè¿‡é…ç½®é€‰æ‹©å™¨æå–åˆ°æ—¥æœŸ: {parsed_date}")
                                        return DateExtractionResult(
                                            success=True,
                                            date=parsed_date,
                                            confidence=0.9,
                                            method=f"configured_selector: {selector}"
                                        )
                        except Exception as e:
                            logger.warning(f"é€‰æ‹©å™¨ {selector} æ‰§è¡Œå¤±è´¥: {e}")
                            continue

                # æ–¹æ³•2: å°è¯•ä»metaæ ‡ç­¾æå–
                meta_result = extract_date_from_meta_tags(soup)
                if meta_result.success:
                    logger.info(f"âœ… ä»Metaæ ‡ç­¾æå–åˆ°æ—¥æœŸ: {meta_result.date}")
                    return meta_result

                # æ–¹æ³•3: å°è¯•ä»JSON-LDç»“æ„åŒ–æ•°æ®æå–
                jsonld_result = extract_date_from_jsonld(soup)
                if jsonld_result.success:
                    logger.info(f"âœ… ä»JSON-LDæå–åˆ°æ—¥æœŸ: {jsonld_result.date}")
                    return jsonld_result

                # æ–¹æ³•4: é€šè¿‡å¸¸è§çš„æ—¶é—´æ ‡ç­¾å’Œç±»åæå–
                common_result = extract_date_from_common_patterns(soup)
                if common_result.success:
                    logger.info(f"âœ… ä»å¸¸è§æ¨¡å¼æå–åˆ°æ—¥æœŸ: {common_result.date}")
                    return common_result

                # æ–¹æ³•5: å°è¯•ä»URLè·¯å¾„ä¸­æå–æ—¥æœŸ
                url_result = extract_date_from_url_path(url)
                if url_result.success:
                    logger.info(f"âœ… ä»URLè·¯å¾„æå–åˆ°æ—¥æœŸ: {url_result.date}")
                    return url_result

                # æ–¹æ³•6: ä»é¡µé¢æ–‡æœ¬ä¸­é€šè¿‡æ¨¡å¼åŒ¹é…æå–
                text_result = extract_date_from_text_patterns(soup.get_text())
                if text_result.success:
                    logger.info(f"âœ… ä»æ–‡æœ¬æ¨¡å¼æå–åˆ°æ—¥æœŸ: {text_result.date}")
                    return text_result

                logger.warning(f"âš ï¸ æœªèƒ½ä»URLæå–åˆ°æ—¥æœŸ: {url}")
                return DateExtractionResult(
                    success=False,
                    error_message="æœªæ‰¾åˆ°å¯è¯†åˆ«çš„æ—¥æœŸä¿¡æ¯"
                )

    except asyncio.TimeoutError:
        return DateExtractionResult(
            success=False,
            error_message="è¯·æ±‚è¶…æ—¶"
        )
    except Exception as e:
        logger.error(f"âŒ æ—¥æœŸæå–å‡ºé”™: {e}")
        return DateExtractionResult(
            success=False,
            error_message=str(e)
        )


def extract_date_from_meta_tags(soup: BeautifulSoup) -> DateExtractionResult:
    """ä»metaæ ‡ç­¾æå–æ—¥æœŸ"""
    meta_selectors = [
        'meta[property="article:published_time"]',
        'meta[property="article:modified_time"]',
        'meta[name="publishdate"]',
        'meta[name="publication_date"]',
        'meta[name="date"]',
        'meta[name="DC.Date"]',
        'meta[name="pubdate"]',
        'meta[itemprop="datePublished"]',
        'meta[itemprop="dateCreated"]'
    ]

    for selector in meta_selectors:
        meta_tag = soup.select_one(selector)
        if meta_tag:
            content = meta_tag.get('content')
            if content:
                parsed_date = parse_date_text(content)
                if parsed_date:
                    return DateExtractionResult(
                        success=True,
                        date=parsed_date,
                        confidence=0.95,
                        method=f"meta_tag: {selector}"
                    )

    return DateExtractionResult(success=False, method="meta_tag")


def extract_date_from_jsonld(soup: BeautifulSoup) -> DateExtractionResult:
    """ä»JSON-LDç»“æ„åŒ–æ•°æ®æå–æ—¥æœŸ"""
    json_scripts = soup.find_all('script', type='application/ld+json')

    for script in json_scripts:
        try:
            data = json.loads(script.string)

            # å¤„ç†æ•°ç»„æ ¼å¼çš„JSON-LD
            if isinstance(data, list):
                data = data[0] if data else {}

            # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
            date_fields = [
                'datePublished', 'dateCreated', 'dateModified',
                'publishedDate', 'createdDate', 'modifiedDate'
            ]

            for field in date_fields:
                if field in data:
                    date_value = data[field]
                    parsed_date = parse_date_text(str(date_value))
                    if parsed_date:
                        return DateExtractionResult(
                            success=True,
                            date=parsed_date,
                            confidence=0.9,
                            method=f"json_ld: {field}"
                        )

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            continue

    return DateExtractionResult(success=False, method="json_ld")


def extract_date_from_common_patterns(soup: BeautifulSoup) -> DateExtractionResult:
    """ä»å¸¸è§çš„HTMLæ¨¡å¼æå–æ—¥æœŸ"""
    # å¸¸è§çš„æ—¶é—´æ ‡ç­¾å’Œå±æ€§
    time_selectors = [
        'time[datetime]',
        'time[pubdate]',
        '.publish-date',
        '.publication-date',
        '.article-date',
        '.post-date',
        '.date-published',
        '.entry-date',
        '.news-date',
        '[class*="date"]',
        '[id*="date"]'
    ]

    for selector in time_selectors:
        try:
            elements = soup.select(selector)
            for element in elements:
                # ä¼˜å…ˆæ£€æŸ¥datetimeå±æ€§
                datetime_attr = element.get('datetime')
                if datetime_attr:
                    parsed_date = parse_date_text(datetime_attr)
                    if parsed_date:
                        return DateExtractionResult(
                            success=True,
                            date=parsed_date,
                            confidence=0.85,
                            method=f"common_pattern_attr: {selector}"
                        )

                # ç„¶åæ£€æŸ¥æ–‡æœ¬å†…å®¹
                text_content = element.get_text(strip=True)
                if text_content:
                    parsed_date = parse_date_text(text_content)
                    if parsed_date:
                        return DateExtractionResult(
                            success=True,
                            date=parsed_date,
                            confidence=0.75,
                            method=f"common_pattern_text: {selector}"
                        )
        except Exception:
            continue

    return DateExtractionResult(success=False, method="common_patterns")


def extract_date_from_url_path(url: str) -> DateExtractionResult:
    """ä»URLè·¯å¾„ä¸­æå–æ—¥æœŸ"""
    try:
        # åŒ¹é…URLä¸­çš„æ—¥æœŸæ¨¡å¼
        date_patterns = [
            r'/(\d{4})/(\d{1,2})/(\d{1,2})/',  # /2024/01/15/
            r'/(\d{4})-(\d{1,2})-(\d{1,2})/',  # /2024-01-15/
            r'/(\d{4})(\d{2})(\d{2})/',  # /20240115/
            r'[?&]date=(\d{4}-\d{1,2}-\d{1,2})',  # ?date=2024-01-15
            r'[?&]year=(\d{4})',  # ?year=2024
        ]

        for pattern in date_patterns:
            match = re.search(pattern, url)
            if match:
                groups = match.groups()
                if len(groups) == 3:  # å¹´æœˆæ—¥
                    year, month, day = groups
                    try:
                        date_obj = datetime(int(year), int(month), int(day))
                        return DateExtractionResult(
                            success=True,
                            date=date_obj.isoformat(),
                            confidence=0.8,
                            method=f"url_path: {pattern}"
                        )
                    except ValueError:
                        continue
                elif len(groups) == 1:  # åªæœ‰å¹´ä»½
                    year = groups[0]
                    if year.isdigit() and 2000 <= int(year) <= 2030:
                        # ä½¿ç”¨å¹´åˆä½œä¸ºé»˜è®¤æ—¥æœŸ
                        date_obj = datetime(int(year), 1, 1)
                        return DateExtractionResult(
                            success=True,
                            date=date_obj.isoformat(),
                            confidence=0.6,
                            method=f"url_path_year: {pattern}"
                        )

    except Exception as e:
        logger.warning(f"URLæ—¥æœŸæå–å¤±è´¥: {e}")

    return DateExtractionResult(success=False, method="url_path")


def extract_date_from_text_patterns(text: str) -> DateExtractionResult:
    """ä»é¡µé¢æ–‡æœ¬ä¸­é€šè¿‡æ¨¡å¼åŒ¹é…æå–æ—¥æœŸ"""
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…å¤„ç†è¿‡é•¿å†…å®¹
    text = text[:5000]

    # æ—¥æœŸæ–‡æœ¬æ¨¡å¼ï¼ˆä¸­è‹±æ–‡ï¼‰
    date_patterns = [
        # ISOæ ¼å¼
        r'(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4}/\d{1,2}/\d{1,2})',

        # ä¸­æ–‡æ ¼å¼
        r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
        r'(\d{4})å¹´(\d{1,2})æœˆ',

        # è‹±æ–‡æ ¼å¼
        r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})',
        r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})',

        # ç›¸å¯¹æ—¶é—´
        r'(\d+)\s*days?\s*ago',
        r'(\d+)\s*hours?\s*ago',
        r'yesterday',
        r'today'
    ]

    for pattern in date_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            matched_text = match.group(0)
            parsed_date = parse_date_text(matched_text)
            if parsed_date:
                return DateExtractionResult(
                    success=True,
                    date=parsed_date,
                    confidence=0.7,
                    method=f"text_pattern: {pattern}"
                )

    return DateExtractionResult(success=False, method="text_pattern")


def parse_date_text(date_text: str) -> Optional[str]:
    """
    ç»Ÿä¸€çš„æ—¥æœŸè§£æå‡½æ•°ï¼Œå°è¯•å¤šç§è§£ææ–¹æ³•

    Args:
        date_text: è¦è§£æçš„æ—¥æœŸæ–‡æœ¬

    Returns:
        è§£ææˆåŠŸè¿”å›ISOæ ¼å¼æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
    """
    if not date_text or not date_text.strip():
        return None

    date_text = date_text.strip()

    try:
        # æ–¹æ³•1: ä½¿ç”¨dateutilçš„æ™ºèƒ½è§£æ
        parsed = dateutil.parser.parse(date_text, fuzzy=True)

        # éªŒè¯æ—¥æœŸåˆç†æ€§ï¼ˆ1990-2030å¹´ä¹‹é—´ï¼‰
        if 1990 <= parsed.year <= 2030:
            return parsed.isoformat()

    except (ValueError, TypeError, OverflowError):
        pass

    try:
        # æ–¹æ³•2: å¤„ç†ä¸­æ–‡æ—¥æœŸæ ¼å¼
        chinese_match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_text)
        if chinese_match:
            year, month, day = chinese_match.groups()
            parsed = datetime(int(year), int(month), int(day))
            return parsed.isoformat()

        chinese_match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ', date_text)
        if chinese_match:
            year, month = chinese_match.groups()
            parsed = datetime(int(year), int(month), 1)
            return parsed.isoformat()
    except (ValueError, TypeError):
        pass

    try:
        # æ–¹æ³•3: å¤„ç†ç›¸å¯¹æ—¶é—´
        if 'ago' in date_text.lower():
            days_match = re.search(r'(\d+)\s*days?\s*ago', date_text, re.IGNORECASE)
            if days_match:
                days_ago = int(days_match.group(1))
                if days_ago <= 365:  # æœ€å¤šä¸€å¹´å‰
                    date_obj = datetime.now() - timedelta(days=days_ago)
                    return date_obj.isoformat()

            hours_match = re.search(r'(\d+)\s*hours?\s*ago', date_text, re.IGNORECASE)
            if hours_match:
                hours_ago = int(hours_match.group(1))
                if hours_ago <= 24 * 7:  # æœ€å¤šä¸€å‘¨å‰
                    date_obj = datetime.now() - timedelta(hours=hours_ago)
                    return date_obj.isoformat()

        if date_text.lower() in ['yesterday', 'æ˜¨å¤©']:
            date_obj = datetime.now() - timedelta(days=1)
            return date_obj.isoformat()

        if date_text.lower() in ['today', 'ä»Šå¤©']:
            date_obj = datetime.now()
            return date_obj.isoformat()

    except (ValueError, TypeError):
        pass

    return None


async def get_article_content_with_date_extraction(article: NewsArticle, date_selectors: List[str] = None) -> tuple[
    ContentFetchResult, Optional[DateExtractionResult]]:
    """
    è·å–æ–‡ç« å†…å®¹å¹¶å°è¯•æå–æ—¥æœŸ

    Returns:
        tuple: (content_result, date_result)
    """
    # é¦–å…ˆè·å–å†…å®¹
    content_result = await get_article_content(article)

    # å¦‚æœæ–‡ç« å·²ç»æœ‰æ—¥æœŸä¸”ä¸å¯ç”¨æ—¥æœŸæå–ï¼Œåˆ™è·³è¿‡
    if not config.ENABLE_DATE_EXTRACTION or (article.publish_date and article.publish_date.strip()):
        return content_result, None

    # å°è¯•ä»URLæå–æ—¥æœŸ
    date_result = await extract_date_from_url(article.url, date_selectors)

    return content_result, date_result


async def get_article_content(article: NewsArticle) -> ContentFetchResult:
    """
    è·å–æ–‡ç« å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨URLæŠ“å–ï¼Œå¤±è´¥æ—¶å›é€€åˆ°ç¼“å­˜å†…å®¹
    """
    # å¦‚æœç¦ç”¨URLæŠ“å–ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜å†…å®¹
    if not config.USE_URL_FETCH:
        return ContentFetchResult(
            success=True,
            content=article.content[:config.CONTENT_MAX_LENGTH],
            source="cached"
        )

    # å°è¯•æŠ“å–URLå†…å®¹
    logger.info(f"æ­£åœ¨æŠ“å–URLå†…å®¹: {article.url}")
    fetch_result = await fetch_webpage_content(article.url)

    if fetch_result.success and len(fetch_result.content.strip()) > 100:
        logger.info(f"âœ… æˆåŠŸæŠ“å–URLå†…å®¹ï¼Œé•¿åº¦: {len(fetch_result.content)}")
        return fetch_result
    else:
        # æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜å†…å®¹ä½œä¸ºå›é€€
        logger.warning(f"âš ï¸ URLæŠ“å–å¤±è´¥ ({fetch_result.error_message})ï¼Œä½¿ç”¨ç¼“å­˜å†…å®¹ä½œä¸ºå›é€€")
        return ContentFetchResult(
            success=True,
            content=article.content[:config.CONTENT_MAX_LENGTH],
            error_message=fetch_result.error_message,
            source="fallback"
        )


# services/ai_service.py - ä¿®æ”¹è¯„åˆ†ç»´åº¦éƒ¨åˆ†

async def analyze_article_with_deepseek(
        article: NewsArticle,
        client_profile: Optional[str] = None,
        date_selectors: List[str] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨DeepSeek APIå¯¹å•ç¯‡æ–‡ç« è¿›è¡Œå…¨é¢çš„AIåˆ†æï¼Œæ”¯æŒæ—¥æœŸæå–
    """
    if not AI_CLIENT:
        raise ConnectionError("AI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œåˆ†æã€‚")

    # è·å–æ–‡ç« å†…å®¹å’Œæ—¥æœŸï¼ˆå¦‚æœéœ€è¦ï¼‰
    content_result, date_result = await get_article_content_with_date_extraction(
        article, date_selectors
    )

    content = content_result.content
    content_source = content_result.source

    # å¦‚æœæˆåŠŸæå–åˆ°æ—¥æœŸï¼Œæ›´æ–°æ–‡ç« å¯¹è±¡
    extracted_date = None
    if date_result and date_result.success:
        extracted_date = date_result.date
        # æ›´æ–°articleå¯¹è±¡çš„æ—¥æœŸï¼ˆç”¨äºåç»­æ•°æ®åº“æ›´æ–°ï¼‰
        article.publish_date = extracted_date
        logger.info(f"ğŸ“… æˆåŠŸä»URLæå–æ—¥æœŸ: {extracted_date} (æ–¹æ³•: {date_result.method})")

    # è·å–å½“å‰æ—¥æœŸ
    from datetime import datetime, timezone
    current_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # æ„å»ºæç¤ºè¯
    topics_str = "ã€".join(config.OFFICIAL_TOPICS)
    categories_str = "\n".join([f"  - **{k}**: {v}" for k, v in config.OFFICIAL_CATEGORIES.items()])
    profile_to_use = client_profile or config.DEFAULT_CLIENT_PROFILE

    # æ ¹æ®å†…å®¹æ¥æºè°ƒæ•´æç¤ºè¯
    content_note = ""
    if content_source == "fetched":
        content_note = "ï¼ˆå†…å®¹æ¥æºï¼šå®æ—¶æŠ“å–çš„æœ€æ–°ç½‘é¡µå†…å®¹ï¼‰"
    elif content_source == "fallback":
        content_note = f"ï¼ˆå†…å®¹æ¥æºï¼šç¼“å­˜å†…å®¹ï¼ŒURLæŠ“å–å¤±è´¥ï¼š{content_result.error_message}ï¼‰"
    else:
        content_note = "ï¼ˆå†…å®¹æ¥æºï¼šç¼“å­˜å†…å®¹ï¼‰"

    # æ—¥æœŸä¿¡æ¯æç¤º
    date_note = ""
    if extracted_date:
        date_note = f"ï¼ˆæ—¥æœŸä¿¡æ¯ï¼šä»URLæå–åˆ°å‘å¸ƒæ—¥æœŸ {extracted_date}ï¼‰"
    elif article.publish_date:
        date_note = f"ï¼ˆæ—¥æœŸä¿¡æ¯ï¼šå·²æœ‰å‘å¸ƒæ—¥æœŸ {article.publish_date}ï¼‰"
    else:
        date_note = "ï¼ˆæ—¥æœŸä¿¡æ¯ï¼šæœªæ‰¾åˆ°å‘å¸ƒæ—¥æœŸï¼‰"

    prompt = f"""ä½ æ˜¯ä¸ºç‰¹å®šä¼ä¸šå®¢æˆ·æœåŠ¡çš„é¦–å¸­å•†ä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚

### å½“å‰æ—¥æœŸ ###
ä»Šå¤©æ˜¯ï¼š{current_date}

### å®¢æˆ·ç”»åƒ (Client Profile) ###
{profile_to_use}

### ä½ çš„æ ¸å¿ƒä»»åŠ¡ ###
åŸºäºä¸Šè¿°**å®¢æˆ·ç”»åƒ**ï¼Œç­›é€‰å’Œè¯„ä¼°é‚£äº›**å¯èƒ½å¯¹è¯¥å®¢æˆ·ä¸šåŠ¡äº§ç”Ÿé‡å¤§å½±å“**çš„ESGæƒ…æŠ¥ã€‚ä½ å¿…é¡»é€»è¾‘ä¸¥è°¨ã€å®¢è§‚ï¼Œå¹¶ä»¥è¯¥å®¢æˆ·çš„è§†è§’ä¸ºä¸­å¿ƒã€‚

### åˆ†æè§„åˆ™ ###
1. **åˆ¤å®šæ—¶æ•ˆæ€§**: æ ¹æ®æƒ…æŠ¥çš„å‘å¸ƒæ—¥æœŸå’Œå½“å‰æ—¥æœŸ({current_date})ï¼Œä¸¥æ ¼åˆ¤æ–­å…¶æ—¶æ•ˆæ€§ï¼Œå¦‚æœæ–‡ç« é‡Œæœ‰è¯´æ˜æ–°é—»å‘ç”Ÿçš„å…·ä½“æ—¶é—´ï¼Œè¯·æŒ‰ç…§é‚£ä¸ªæ—¶é—´è¯„ä»·æ—¶æ•ˆæ€§ã€‚
2. **è¯„ä¼°å¯é æ€§**: åŸºäºæƒ…æŠ¥çš„æ¥æºï¼Œè¯„ä¼°å…¶å¯ä¿¡åº¦ã€‚
3. **è¡¡é‡ä¸šåŠ¡å½±å“**: è¿™æ˜¯æœ€é‡è¦çš„ç»´åº¦ã€‚æ·±å…¥åˆ†ææƒ…æŠ¥å†…å®¹ï¼Œåˆ¤æ–­å…¶å¯¹å®¢æˆ·ä¸šåŠ¡å†³ç­–çš„æ½œåœ¨å½±å“ã€‚
4. **ç”Ÿæˆæ‘˜è¦**: ç”Ÿæˆä¸€æ®µä¸è¶…è¿‡200å­—çš„ã€å®¢è§‚çš„æƒ…æŠ¥æ‘˜è¦ï¼Œä¸ä¼šå› ä¸ºå®¢æˆ·ä¸šåŠ¡è€Œæ”¹å˜ã€‚
5. **ç”Ÿæˆæ–°æ ‡é¢˜**: åŸºäºæ–‡ç« å†…å®¹ç”Ÿæˆä¸€ä¸ªæ›´å‡†ç¡®ã€ç®€æ´çš„æ–°æ ‡é¢˜æ¥æ›¿æ¢åŸæ ‡é¢˜ã€‚
6. **ä¸¥æ ¼æ ¼å¼åŒ–**: å¿…é¡»ä»¥æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–è¯´æ˜ã€‚

### è¯„åˆ†ç»´åº¦å®šä¹‰ ###
- **æˆ˜ç•¥ç›¸å…³æ€§ (æƒé‡30%)**: æƒ…æŠ¥ä¸å®¢æˆ·æˆ˜ç•¥ç›®æ ‡ã€é‡ç‚¹å¸‚åœºã€æ ¸å¿ƒäº§å“çº¿çš„åŒ¹é…åº¦
  - 1-3åˆ†: åŸºæœ¬æ— å…³æˆ–æ¬¡è¦å…³è”
  - 4-6åˆ†: ä¸å®¢æˆ·æŸä¸ªä¸šåŠ¡æ¿å—æœ‰ä¸­ç­‰ç›¸å…³æ€§
  - 7-10åˆ†: ç›´æ¥æ¶‰åŠå®¢æˆ·æ ¸å¿ƒæˆ˜ç•¥æˆ–ä¸»è¥ä¸šåŠ¡
- **è¡Œä¸šå½±å“åŠ› (æƒé‡20%)**: äº‹ä»¶å¯¹è¡Œä¸šæ ¼å±€çš„æ½œåœ¨å½±å“
  - 1-3åˆ†: å½±å“æå°ï¼Œä»…ä¸ºè¡Œä¸šèƒŒæ™¯ä¿¡æ¯
  - 4-6åˆ†: è¡Œä¸šå†…å€¼å¾—å…³æ³¨çš„äº‹ä»¶
  - 7-10åˆ†: å¯èƒ½æ”¹å˜è¡Œä¸šæ ¼å±€æˆ–å‘å±•è¶‹åŠ¿çš„é‡å¤§äº‹ä»¶
- **æ—¶æ•ˆæ€§ç´§è¿«æ€§ (æƒé‡20%)**: æƒ…æŠ¥çš„æ—¶æ•ˆä»·å€¼å’Œç´§è¿«æ€§
  - è®¡ç®—æ–¹æ³•ï¼šå°†å‘å¸ƒæ—¥æœŸä¸å½“å‰æ—¥æœŸ({current_date})å¯¹æ¯”
  - å¦‚æœå‘å¸ƒæ—¥æœŸåœ¨è¿‡å»7å¤©å†…ï¼šå¾—8-10åˆ†
  - å¦‚æœå‘å¸ƒæ—¥æœŸåœ¨è¿‡å»30å¤©å†…ï¼šå¾—5-7åˆ†
  - å¦‚æœå‘å¸ƒæ—¥æœŸè¶…è¿‡30å¤©ï¼šå¾—1-4åˆ†
  - å¦‚æœæ˜¯æœªæ¥çš„é‡è¦æ—¶é—´ç»™8-10åˆ†
- **ä¸šåŠ¡æœºä¼šé£é™©å¼ºåº¦ (æƒé‡15%)**: è¯¥æƒ…æŠ¥å¯¹å®¢æˆ·å¯èƒ½å¸¦æ¥çš„æœºä¼šæˆ–é£é™©å¼ºå¼±
  - 1-3åˆ†: å‡ ä¹æ— å®é™…å•†ä¸šå½±å“
  - 4-6åˆ†: å­˜åœ¨ä¸­ç­‰ç¨‹åº¦çš„æœºä¼šæˆ–å¨èƒ
  - 7-10åˆ†: å¯èƒ½æ˜¾è‘—æ”¹å˜å®¢æˆ·æ”¶ç›Šã€æˆæœ¬æˆ–å¸‚åœºåœ°ä½
- **å¯æ“ä½œæ€§ (æƒé‡15%)**: æƒ…æŠ¥èƒ½å¦è½¬åŒ–ä¸ºæ˜ç¡®çš„è¡ŒåŠ¨å»ºè®®
  - 1-3åˆ†: çº¯èƒŒæ™¯ä¿¡æ¯ï¼Œæ— æ˜ç¡®è¡ŒåŠ¨æŒ‡å‘
  - 4-6åˆ†: æœ‰ä¸€å®šå¯å‘ä»·å€¼ï¼Œå¯ä½œä¸ºå†³ç­–å‚è€ƒ
  - 7-10åˆ†: å¯ç›´æ¥è½åœ°æ‰§è¡Œæˆ–åˆ¶å®šå…·ä½“åº”å¯¹æ–¹æ¡ˆ

### å®˜æ–¹åˆ—è¡¨ ###
å®˜æ–¹è®®é¢˜: {topics_str}
è®®é¢˜è¯´æ˜ï¼š
å®˜æ–¹ç±»åˆ«:
{categories_str}

### å¾…åˆ†æçš„æƒ…æŠ¥åŸæ–‡ ###
å‘å¸ƒæ—¥æœŸ: {article.publish_date}
æ ‡é¢˜: {article.title}
æ¥æºURL: {article.url}
å†…å®¹ {content_note}{date_note}: {content}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
    "è®®é¢˜": "<ä»å®˜æ–¹åˆ—è¡¨ä¸­é€‰æ‹©æœ€è´´åˆ‡çš„è®®é¢˜>",
    "ç±»åˆ«": "<ä»å®˜æ–¹åˆ—è¡¨ä¸­é€‰æ‹©æœ€è´´åˆ‡çš„ç±»åˆ«>",
    "æ‘˜è¦": "<æƒ…æŠ¥æ‘˜è¦ï¼Œå®¢è§‚å³å¯ï¼Œæ— éœ€é¢å¯¹å®¢æˆ·è¯„ä»·ï¼Œ150-200å­—ï¼Œæ ¼å¼éœ€è¦æŒ‰ç…§XXäºï¼ˆæ—¶é—´ï¼Œå…·ä½“åˆ°æ—¥ï¼‰åšäº†XX>",
    "æ–°æ ‡é¢˜": "<æ ¹æ®æ–‡ç« å†…å®¹ç”Ÿæˆæ›´å‡†ç¡®ç®€æ´çš„æ–°æ ‡é¢˜ï¼Œä¸è¶…è¿‡50å­—>",
    "è¯„åˆ†è¯¦æƒ…": {{
        "æˆ˜ç•¥ç›¸å…³æ€§": {{ "åˆ†æ•°": <0-10æ•´æ•°>, "ç†ç”±": "<æ‰“åˆ†ç†ç”±>" }},
        "è¡Œä¸šå½±å“åŠ›": {{ "åˆ†æ•°": <0-10æ•´æ•°>, "ç†ç”±": "<æ‰“åˆ†ç†ç”±>" }},
        "æ—¶æ•ˆæ€§ç´§è¿«æ€§": {{ "åˆ†æ•°": <0-10æ•´æ•°>, "ç†ç”±": "<åŸºäºå½“å‰æ—¥æœŸ{current_date}çš„æ—¶æ•ˆæ€§åˆ¤æ–­ç†ç”±>" }},
        "ä¸šåŠ¡æœºä¼šé£é™©å¼ºåº¦": {{ "åˆ†æ•°": <0-10æ•´æ•°>, "ç†ç”±": "<æ‰“åˆ†ç†ç”±>" }},
        "å¯æ“ä½œæ€§": {{ "åˆ†æ•°": <0-10æ•´æ•°>, "ç†ç”±": "<æ‰“åˆ†ç†ç”±>" }}
    }}
}}"""

    try:
        response = await AI_CLIENT.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )

        output_text = response.choices[0].message.content
        logger.info(f"DeepSeek API åŸå§‹è¿”å›: {output_text[:200]}...")

        analysis_results = _parse_ai_output(output_text)

        if _validate_analysis_result(analysis_results):
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            meta_info = {
                "content_source": content_source,
                "content_length": len(content),
                "fetch_error": content_result.error_message if content_result.error_message else None,
                "analysis_date": current_date
            }

            # å¦‚æœæå–åˆ°äº†æ—¥æœŸï¼Œæ·»åŠ æ—¥æœŸæå–ä¿¡æ¯
            if date_result:
                meta_info.update({
                    "date_extracted": date_result.success,
                    "extracted_date": date_result.date if date_result.success else None,
                    "date_extraction_method": date_result.method if date_result.success else None,
                    "date_confidence": date_result.confidence if date_result.success else None
                })

            analysis_results["_meta"] = meta_info
            logger.info(f"AIæˆåŠŸåˆ†ææ–‡ç« : {article.title[:30]}... (å†…å®¹æ¥æº: {content_source})")
            return analysis_results
        else:
            logger.error(f"AIè¿”å›äº†æ— æ•ˆçš„åˆ†æç»“æœ: {article.title[:30]}...")
            # è¿”å›é»˜è®¤ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                "è®®é¢˜": config.OFFICIAL_TOPICS[0],
                "ç±»åˆ«": list(config.OFFICIAL_CATEGORIES.keys())[0],
                "æ‘˜è¦": article.title,
                "æ–°æ ‡é¢˜": article.title[:50],
                "è¯„åˆ†è¯¦æƒ…": {
                    "æˆ˜ç•¥ç›¸å…³æ€§": {"åˆ†æ•°": 0, "ç†ç”±": "é»˜è®¤è¯„åˆ†"},
                    "è¡Œä¸šå½±å“åŠ›": {"åˆ†æ•°": 0, "ç†ç”±": "é»˜è®¤è¯„åˆ†"},
                    "æ—¶æ•ˆæ€§ç´§è¿«æ€§": {"åˆ†æ•°": 0, "ç†ç”±": "é»˜è®¤è¯„åˆ†"},
                    "ä¸šåŠ¡æœºä¼šé£é™©å¼ºåº¦": {"åˆ†æ•°": 0, "ç†ç”±": "é»˜è®¤è¯„åˆ†"},
                    "å¯æ“ä½œæ€§": {"åˆ†æ•°": 0, "ç†ç”±": "é»˜è®¤è¯„åˆ†"}
                },
                "_meta": {
                    "content_source": content_source,
                    "content_length": len(content),
                    "fetch_error": "AIåˆ†æç»“æœéªŒè¯å¤±è´¥",
                    "analysis_date": current_date,
                    "date_extracted": date_result.success if date_result else False,
                    "extracted_date": date_result.date if date_result and date_result.success else None
                }
            }

    except Exception as e:
        logger.error(f"è°ƒç”¨DeepSeek APIæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        # è¿”å›é»˜è®¤ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
        return {
            "è®®é¢˜": config.OFFICIAL_TOPICS[0],
            "ç±»åˆ«": list(config.OFFICIAL_CATEGORIES.keys())[0],
            "æ‘˜è¦": f"åˆ†æå¤±è´¥: {article.title}",
            "æ–°æ ‡é¢˜": article.title[:50],
            "è¯„åˆ†è¯¦æƒ…": {
                "æˆ˜ç•¥ç›¸å…³æ€§": {"åˆ†æ•°": 0, "ç†ç”±": f"APIè°ƒç”¨å¤±è´¥: {str(e)[:50]}"},
                "è¡Œä¸šå½±å“åŠ›": {"åˆ†æ•°": 0, "ç†ç”±": "APIè°ƒç”¨å¤±è´¥"},
                "æ—¶æ•ˆæ€§ç´§è¿«æ€§": {"åˆ†æ•°": 0, "ç†ç”±": "APIè°ƒç”¨å¤±è´¥"},
                "ä¸šåŠ¡æœºä¼šé£é™©å¼ºåº¦": {"åˆ†æ•°": 0, "ç†ç”±": "APIè°ƒç”¨å¤±è´¥"},
                "å¯æ“ä½œæ€§": {"åˆ†æ•°": 0, "ç†ç”±": "APIè°ƒç”¨å¤±è´¥"}
            },
            "_meta": {
                "content_source": content_source,
                "content_length": len(content) if 'content' in locals() else 0,
                "fetch_error": f"APIè°ƒç”¨å¤±è´¥: {str(e)}",
                "analysis_date": current_date,
                "date_extracted": False,
                "extracted_date": None
            }
        }


def _validate_analysis_result(result: Dict) -> bool:
    """
    éªŒè¯AIè¿”å›çš„ç»“æœæ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„è§„èŒƒã€‚
    """
    if not result:
        return False

    required_keys = ["è®®é¢˜", "ç±»åˆ«", "æ‘˜è¦", "æ–°æ ‡é¢˜", "è¯„åˆ†è¯¦æƒ…"]
    if not all(k in result for k in required_keys):
        logger.warning(f"AIåˆ†æç»“æœç¼ºå°‘å¿…è¦å­—æ®µ: {[k for k in required_keys if k not in result]}")
        return False

    if result.get("è®®é¢˜") not in config.OFFICIAL_TOPICS:
        logger.warning(f"AIè¿”å›äº†æœªçŸ¥çš„è®®é¢˜: {result.get('è®®é¢˜')}")
        return False

    if result.get("ç±»åˆ«") not in config.OFFICIAL_CATEGORIES:
        logger.warning(f"AIè¿”å›äº†æœªçŸ¥çš„ç±»åˆ«: {result.get('ç±»åˆ«')}")
        return False

    score_details = result.get("è¯„åˆ†è¯¦æƒ…")
    if not isinstance(score_details, dict) or not score_details:
        logger.warning("AIè¿”å›çš„è¯„åˆ†è¯¦æƒ…æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return False

    # éªŒè¯è¯„åˆ†è¯¦æƒ…çš„ç»“æ„
    required_score_keys = ["æˆ˜ç•¥ç›¸å…³æ€§", "è¡Œä¸šå½±å“åŠ›", "æ—¶æ•ˆæ€§ç´§è¿«æ€§", "ä¸šåŠ¡æœºä¼šé£é™©å¼ºåº¦", "å¯æ“ä½œæ€§"]
    for score_key in required_score_keys:
        if score_key not in score_details:
            logger.warning(f"è¯„åˆ†è¯¦æƒ…ä¸­ç¼ºå°‘ {score_key}")
            return False
        score_item = score_details[score_key]
        if not isinstance(score_item, dict) or "åˆ†æ•°" not in score_item:
            logger.warning(f"{score_key} çš„è¯„åˆ†æ ¼å¼ä¸æ­£ç¡®")
            return False

    return True


def _parse_ai_output(output_text: str) -> Dict[str, Any]:
    """
    ä»AIè¿”å›çš„æ–‡æœ¬ä¸­å®‰å…¨åœ°è§£æå‡ºJSONå¯¹è±¡ã€‚
    """
    try:
        # å…ˆå°è¯•ç›´æ¥è§£æJSON
        if output_text.strip().startswith('{'):
            return json.loads(output_text)

        # å†å°è¯•ä»ä»£ç å—ä¸­æå–
        match = re.search(r"```json\s*([\s\S]+?)\s*```", output_text)
        if match:
            json_str = match.group(1)
        else:
            json_str = output_text

        # æŸ¥æ‰¾JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
        start = json_str.find('{')
        end = json_str.rfind('}') + 1

        if start != -1 and end != 0:
            return json.loads(json_str[start:end])
        return {}

    except (json.JSONDecodeError, IndexError, AttributeError) as e:
        logger.error(f"è§£æAIè¾“å‡ºçš„JSONå¤±è´¥: {e}\nåŸå§‹è¾“å‡º: {output_text[:300]}...")
        return {}


# è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨å¤„ç†æ—¥æœŸå­—æ®µ
def safe_get_publish_date(news_time) -> str:
    """
    å®‰å…¨åœ°è·å–å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸²
    """
    if hasattr(news_time, 'isoformat'):
        # datetime å¯¹è±¡
        return news_time.isoformat()
    elif isinstance(news_time, str):
        # å­—ç¬¦ä¸²
        return news_time
    else:
        # å…¶ä»–ç±»å‹æˆ–None
        return ""


# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®å‡½æ•°
def set_url_fetch_enabled(enabled: bool):
    """
    å¯ç”¨æˆ–ç¦ç”¨URLæŠ“å–åŠŸèƒ½
    """
    config.USE_URL_FETCH = enabled
    logger.info(f"URLæŠ“å–åŠŸèƒ½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")


def set_date_extraction_enabled(enabled: bool):
    """
    å¯ç”¨æˆ–ç¦ç”¨æ·±åº¦æ—¥æœŸæå–åŠŸèƒ½
    """
    config.ENABLE_DATE_EXTRACTION = enabled
    logger.info(f"æ·±åº¦æ—¥æœŸæå–åŠŸèƒ½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")


def set_fetch_timeout(timeout: int):
    """
    è®¾ç½®URLæŠ“å–è¶…æ—¶æ—¶é—´
    """
    config.FETCH_TIMEOUT = timeout
    logger.info(f"URLæŠ“å–è¶…æ—¶æ—¶é—´è®¾ç½®ä¸º {timeout} ç§’")


# æ–°å¢ï¼šæ‰¹é‡æ—¥æœŸè¡¥å……åŠŸèƒ½
async def batch_extract_missing_dates(
        articles: List[Dict[str, Any]],
        date_selectors: List[str] = None
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡ä¸ºç¼ºå°‘æ—¥æœŸçš„æ–‡ç« æå–æ—¥æœŸ

    Args:
        articles: æ–‡ç« åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« åº”åŒ…å« id, url, title ç­‰å­—æ®µ
        date_selectors: å¯é€‰çš„æ—¥æœŸé€‰æ‹©å™¨åˆ—è¡¨

    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«æˆåŠŸæå–çš„æ—¥æœŸä¿¡æ¯
    """
    logger.info(f"ğŸ• å¼€å§‹æ‰¹é‡æ—¥æœŸæå–ï¼Œå…± {len(articles)} ç¯‡æ–‡ç« ")
    results = []

    # è¿‡æ»¤å‡ºæ²¡æœ‰æ—¥æœŸçš„æ–‡ç« 
    articles_without_date = [
        article for article in articles
        if not article.get('news_time') or not str(article.get('news_time')).strip()
    ]

    logger.info(f"ğŸ“Š å‘ç° {len(articles_without_date)} ç¯‡æ–‡ç« ç¼ºå°‘æ—¥æœŸ")

    for i, article in enumerate(articles_without_date, 1):
        try:
            logger.info(f"â³ [{i}/{len(articles_without_date)}] å¤„ç†æ–‡ç« : {article.get('title', 'æ— æ ‡é¢˜')[:50]}...")

            url = article.get('url')
            if not url:
                results.append({
                    'id': article.get('id'),
                    'success': False,
                    'error': 'URLä¸ºç©º'
                })
                continue

            # æå–æ—¥æœŸ
            date_result = await extract_date_from_url(url, date_selectors)

            if date_result.success:
                results.append({
                    'id': article.get('id'),
                    'success': True,
                    'extracted_date': date_result.date,
                    'method': date_result.method,
                    'confidence': date_result.confidence,
                    'url': url
                })
                logger.info(f"âœ… æˆåŠŸæå–: {date_result.date} (ç½®ä¿¡åº¦: {date_result.confidence:.2f})")
            else:
                results.append({
                    'id': article.get('id'),
                    'success': False,
                    'error': date_result.error_message,
                    'url': url
                })
                logger.warning(f"âŒ æå–å¤±è´¥: {date_result.error_message}")

            # é¿å…è¿‡å¿«è¯·æ±‚
            if i < len(articles_without_date):
                await asyncio.sleep(0.5)

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
            results.append({
                'id': article.get('id'),
                'success': False,
                'error': str(e),
                'url': article.get('url')
            })

    success_count = sum(1 for r in results if r['success'])
    logger.info(f"ğŸ‰ æ‰¹é‡æ—¥æœŸæå–å®Œæˆ: æˆåŠŸ {success_count}/{len(results)} ç¯‡")

    return results